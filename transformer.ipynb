{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import log_softmax\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "   \"\"\"\n",
    "   标准的编码器-解码器架构。该架构是许多模型的基础。\n",
    "   - encoder：编码器模块，用于处理输入（源）序列。\n",
    "   - decoder：解码器模块，用于生成输出（目标）序列。\n",
    "   - src_embed 和 tgt_embed：分别用于对源序列和目标序列进行嵌入（embedding）的模块，将输入序列转化为向量表示。\n",
    "   - generator：通常是一个全连接层，用于将解码器的输出转化为最终的预测结果，例如词汇表中的概率分布。\n",
    "   \"\"\"\n",
    "\n",
    "   def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "      super(EncoderDecoder, self).__init__()\n",
    "      self.encoder = encoder\n",
    "      self.decoder = decoder\n",
    "      self.src_embed = src_embed\n",
    "      self.tgt_embed = tgt_embed\n",
    "      self.generator = generator\n",
    "\n",
    "   def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "      # 接收并处理 marked 的 src 和目标序列。\n",
    "      return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "\n",
    "   def encode(self, src, src_mask):\n",
    "      # 将 src 序列进行编码\n",
    "      return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "   def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "      # 将 memory 和 tgt 序列进行解码\n",
    "      return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "   \"定义标准线性softmax生成步骤。\"\n",
    "\n",
    "   def __init__(self, d_model, vocab):\n",
    "      super(Generator, self).__init__()\n",
    "      # 这个线性层将输入从 d_model 维度投影到 vocab 维度\n",
    "      self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "   def forward(self, x):\n",
    "      return log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder   \n",
    "将 N 个相同的子层堆叠在一起，形成一个编码器或解码器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"生成 N 个相同的 layer。\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"构造一个层归一化（LayerNorm）。\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        # 初始化3个可训练参数，用于缩放和平移\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"编码器由 N 个相同的层堆叠组成。\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        # 将 layer 克隆 N 次\n",
    "        self.layers = clones(layer, N)\n",
    "        # 层归一化\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        依次通过每个层传递输入 (和掩码)。\n",
    "        通过每个层运行 x，然后进行规范化。\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"编码器由自注意力机制和前馈网络 (定义如下) 组成\"\n",
    "\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn # self_attn将在下面的代码实现\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "       \"生成具有掩码的通用N层解码器。\"\n",
    "       def __init__(self, layer, N):\n",
    "              super(Decoder, self).__init__()\n",
    "              self.layers = clones(layer, N)\n",
    "              self.norm = LayerNorm(layer.size)\n",
    "\n",
    "       def forward(self, x, memory, src_mask, tgt_mask):\n",
    "              \"\"\"\n",
    "              - x：目标序列的嵌入表示。\n",
    "              - memory：来自编码器的输出，包含源序列的信息。\n",
    "              - src_mask：源序列的掩码，用于源注意力机制。\n",
    "              - tgt_mask：目标序列的掩码，用于自注意力机制，确保自回归性质。\n",
    "              \"\"\"\n",
    "              for layer in self.layers:\n",
    "                     x = layer(x, memory, src_mask, tgt_mask)\n",
    "              return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "       \"\"\"\n",
    "       解码器层，由三个主要部分组成：自注意力、源注意力和前馈神经网络。\n",
    "       \"\"\"\n",
    "\n",
    "       def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "              \"\"\"\n",
    "              初始化解码器层。\n",
    "\n",
    "              参数:\n",
    "              - size: 模型的维度\n",
    "              - self_attn: 自注意力机制\n",
    "              - src_attn: 源注意力机制（也称为交叉注意力）\n",
    "              - feed_forward: 前馈神经网络\n",
    "              - dropout: Dropout 比率\n",
    "              \"\"\"\n",
    "              super(DecoderLayer, self).__init__()\n",
    "              self.size = size\n",
    "              self.self_attn = self_attn  \n",
    "              self.src_attn = src_attn   \n",
    "              self.feed_forward = feed_forward  # 前馈神经网络\n",
    "              # 创建三个子层连接，用于实现残差连接和层归一化\n",
    "              self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "       def forward(self, x, memory, src_mask, tgt_mask):\n",
    "              \"\"\"\n",
    "              前向传播函数。\n",
    "\n",
    "              参数:\n",
    "              - x: 解码器的输入\n",
    "              - memory: 编码器的输出\n",
    "              - src_mask: 源序列的掩码\n",
    "              - tgt_mask: 目标序列的掩码\n",
    "\n",
    "              返回:\n",
    "              - 经过完整解码器层处理后的张量\n",
    "              \"\"\"\n",
    "              m = memory\n",
    "              # 1. 自注意力子层\n",
    "              # 这里的 x 为解码器的输入，x 被用作 query、key 和 value。\n",
    "              x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "              # 2. 源注意力子层（交叉注意力）\n",
    "              # 这里 x 是来自解码器的查询（query），而 m（即 memory）是来自编码器的键（key）和值（value）\n",
    "              x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "              # 3. 前馈神经网络子层\n",
    "              return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"创建一个掩码来屏蔽后续位置。\"\n",
    "    # 创建一个形状为 (1, size, size) 的三维张量\n",
    "    attn_shape = (1, size, size)\n",
    "    \n",
    "    # 使用 torch.triu 创建一个上三角矩阵，对角线上移一位\n",
    "    # diagonal=1 表示主对角线上方的第一条对角线\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(torch.uint8)\n",
    "    \n",
    "    # 将上三角矩阵取反，得到最终的掩码\n",
    "    return subsequent_mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False, False],\n",
       "         [ True,  True, False, False, False],\n",
       "         [ True,  True,  True, False, False],\n",
       "         [ True,  True,  True,  True, False],\n",
       "         [ True,  True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsequent_mask(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
